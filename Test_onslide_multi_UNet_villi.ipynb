{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd173b74-e1fd-49c1-87db-35d24405c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gzip\n",
    "import numpy as np\n",
    "import openslide\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import cv2\n",
    "import shapely\n",
    "import geojson\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.strtree import STRtree\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from villi_augment import *\n",
    "from UNet_villi import *\n",
    "\n",
    "# Ensure this is set before torch is imported if it's relevant for your environment\n",
    "# os.environ['TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb1eeb8-fb2b-421c-801b-ec1ca8c10323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qupath_color_to_rgb(n):\n",
    "    if n < 0:\n",
    "        n = (1 << 24) + n\n",
    "\n",
    "    red = (n >> 16) & 0xFF\n",
    "    green = (n >> 8) & 0xFF\n",
    "    blue = n & 0xFF\n",
    "\n",
    "    return red, green, blue\n",
    "\n",
    "\n",
    "def rgb_to_qupath_color(red, green, blue, alpha=255):\n",
    "    unsigned_int = (alpha << 24) | (red << 16) | (green << 8) | blue\n",
    "    signed_int = unsigned_int if unsigned_int < 0x80000000 else unsigned_int - 0x100000000\n",
    "    return signed_int\n",
    "\n",
    "\n",
    "def divide_batch(l, n):\n",
    "    for i in range(0, l.shape[0], n):\n",
    "        yield l[i:i + n,::]\n",
    "\n",
    "\n",
    "def _DFS(polygons, contours, hierarchy, sibling_id, is_outer, siblings):\n",
    "    while sibling_id != -1:\n",
    "        contour = contours[sibling_id].squeeze(axis=1)\n",
    "        if len(contour) >= 3:\n",
    "            first_child_id = hierarchy[sibling_id][2]\n",
    "            children = [] if is_outer else None\n",
    "            _DFS(polygons, contours, hierarchy, first_child_id, not is_outer, children)\n",
    "\n",
    "            if is_outer:\n",
    "                polygon = Polygon(contour, holes=children)\n",
    "                polygons.append(polygon)\n",
    "            else:\n",
    "                siblings.append(contour)\n",
    "        sibling_id = hierarchy[sibling_id][0]\n",
    "\n",
    "\n",
    "def generate_polygons(contours, hierarchy):\n",
    "    hierarchy = hierarchy[0]\n",
    "    polygons = []\n",
    "    _DFS(polygons, contours, hierarchy, 0, True, [])\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def generate_polygons_from_mask(mask, coords, scalefactor):\n",
    "    x, y = coords[0], coords[1]\n",
    "    # Ensure mask is 8-bit unsigned integer type for cv2.findContours\n",
    "    contours, hierarchy = cv2.findContours(mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    if hierarchy is not None:\n",
    "        polygons = generate_polygons(contours, hierarchy)\n",
    "        # Apply scaling and offset to polygon coordinates\n",
    "        polygons = [shapely.Polygon(shell=poly.exterior.coords._coords * scalefactor + np.array([x, y]),\n",
    "                                    holes=[interior.coords._coords * scalefactor + np.array([x, y]) for interior in poly.interiors])\n",
    "                    for poly in polygons]\n",
    "\n",
    "    # print(f'Number of polygons found: {len(polygons)}')\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def predict_without_padding(tile, model, device):\n",
    "    # prepare data appropriately\n",
    "    img_gpu = torch.from_numpy(np.expand_dims(tile, axis=0)).to(device, memory_format=torch.channels_last)\n",
    "    img_gpu = img_gpu.permute(0, 3, 1, 2) / 255\n",
    "    img_gpu = img_gpu.type(torch.float32)\n",
    "\n",
    "    # apply unet to current patch\n",
    "    unet_out_ext = model(img_gpu)\n",
    "    unet_out_ext = unet_out_ext.detach().cpu().numpy()\n",
    "    unet_out = np.squeeze(unet_out_ext, axis=0)\n",
    "\n",
    "    # get mask (unet returns the values before softmax)\n",
    "    unet_out = np.argmax(unet_out, axis=0)\n",
    "\n",
    "    return unet_out\n",
    "\n",
    "\n",
    "def predict_with_padding(tile, model, device):\n",
    "    stride = 256\n",
    "    tile_shape = tile.shape\n",
    "\n",
    "    # prepare data appropriately\n",
    "    img_gpu = torch.from_numpy(np.expand_dims(tile, axis=0)).to(device, memory_format=torch.channels_last)\n",
    "    img_gpu = torch.nn.functional.pad(img_gpu, (0, 0, stride // 2, stride // 2, stride // 2, stride // 2), mode='reflect')\n",
    "    img_gpu = img_gpu.permute(0, 3, 1, 2) / 255\n",
    "    img_gpu = img_gpu.type(torch.float32)\n",
    "\n",
    "    # apply unet to current patch\n",
    "    unet_out_ext = model(img_gpu)\n",
    "    unet_out_ext = unet_out_ext[:, :, stride // 2:, stride // 2:][:, :, :tile_shape[0], :tile_shape[1]]\n",
    "    unet_out_ext = unet_out_ext.detach().cpu().numpy()\n",
    "    unet_out = np.squeeze(unet_out_ext, axis=0)\n",
    "\n",
    "    # get mask (unet returns the values before softmax)\n",
    "    unet_out = np.argmax(unet_out, axis=0)\n",
    "\n",
    "    return unet_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f15272-4e2f-4b01-a724-e878af25d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openslide level: 3, tile size: 1024\n",
      "Exploring folder D:\\Molar_pregnancy\\molar_data\\tif_slides\n",
      "['D:\\\\Molar_pregnancy\\\\molar_data\\\\tif_slides/DP00002010.tif']\n",
      "Saving debug masks to: D:\\Molar_pregnancy\\Aron_to_share\\output_masks\n",
      "total params: \t487307\n",
      "Opening image D:\\Molar_pregnancy\\molar_data\\tif_slides/DP00002010.tif which corresponds to image named DP00002010\n",
      "Starting time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ": 100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [01:14<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing union...\n",
      "Doing union...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|██████████████████████████████████████████████████████████████████████████████| 6512/6512 [06:11<00:00, 17.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done saving annotations\n",
      "Elapsed time for image D:\\Molar_pregnancy\\molar_data\\tif_slides/DP00002010.tif : 506.81022596359253\n",
      "Done exploring all files!\n"
     ]
    }
   ],
   "source": [
    "# set level and tilesize\n",
    "\n",
    "openslidelevel = 3\n",
    "tilesize = 1024\n",
    "print(f\"Openslide level: {openslidelevel}, tile size: {tilesize}\")\n",
    "\n",
    "#data for molar pregnancy slides\n",
    "data_folder = \"D:\\\\Molar_pregnancy\\\\molar_data\\\\tif_slides\"\n",
    "molar_type = ['P', 'P', 'P', 'P', 'C', 'C', 'C', 'C', 'N', 'N', 'N']\n",
    "staining_type = ['HE', 'HE', 'HE', 'P57', 'HE', 'HE', 'HE', 'P57', 'HE', 'HE', 'P57']\n",
    "\n",
    "print(f\"Exploring folder {data_folder}\")\n",
    "\n",
    "#select single image\n",
    "single_img = 10 # <- change here!\n",
    "\n",
    "imgs = [data_folder + '/DP000020'+str(single_img).zfill(2) + '.tif']\n",
    "\n",
    "print(imgs)\n",
    "\n",
    "# Create directory for debug masks\n",
    "debug_mask_dir = \"D:\\\\Molar_pregnancy\\\\Aron_to_share\\\\output_masks\"\n",
    "os.makedirs(debug_mask_dir, exist_ok=True)\n",
    "print(f\"Saving debug masks to: {debug_mask_dir}\")\n",
    "\n",
    "# load model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "model_path = f'D:\\\\Molar_pregnancy\\\\Aron_to_share\\\\Models\\\\data_villi_multi_best_model_multi_UNet.pth'\n",
    "\n",
    "# Check if model path exists before loading\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Error: Model file not found at {model_path}. Please check the path.\")\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model = UNet(n_classes=checkpoint[\"n_classes\"], in_channels=checkpoint[\"in_channels\"],\n",
    "         padding=checkpoint[\"padding\"], depth=checkpoint[\"depth\"], wf=checkpoint[\"wf\"],\n",
    "         up_mode=checkpoint[\"up_mode\"], batch_norm=checkpoint[\"batch_norm\"]).to(device)\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#loop over selected images\n",
    "for img in imgs:\n",
    "    img_name = os.path.basename(img).split('.')[0]\n",
    "    img_num = int(img_name[-2:])\n",
    "    print(f'Opening image {img} which corresponds to image named {img_name}')\n",
    "    \n",
    "    print('Starting time...')\n",
    "    start = time.time()\n",
    "    \n",
    "    osh = openslide.OpenSlide(img)\n",
    "    \n",
    "    n_row, n_col = osh.level_dimensions[0]\n",
    "    \n",
    "    scalefactor = osh.level_downsamples[openslidelevel]\n",
    "    \n",
    "    # global polygons\n",
    "    allinternalpolygons_out = []\n",
    "    allboundarypolygons_out = []\n",
    "    \n",
    "    # loop over tiles\n",
    "    for y in tqdm(range(0, osh.level_dimensions[0][1], round(tilesize * scalefactor)), desc='y'):\n",
    "        for x in range(0, osh.level_dimensions[0][0], round(tilesize * scalefactor)):\n",
    "    \n",
    "            tilepoly = Polygon([[x, y], [x + tilesize * scalefactor, y], [x + tilesize * scalefactor, y + tilesize * scalefactor], [x, y + tilesize * scalefactor]])\n",
    "    \n",
    "            tile = np.array(osh.read_region((x, y), openslidelevel, (tilesize, tilesize)))[:, :, 0:3]\n",
    "    \n",
    "            # boundary fix to set padding of 255 instead of 0\n",
    "            if x + tilesize * scalefactor >= osh.level_dimensions[0][0]:\n",
    "                valid_x = int((osh.level_dimensions[0][0] - 1 - x) // scalefactor)\n",
    "                tile[:, valid_x:, :] = 255\n",
    "            if y + tilesize * scalefactor >= osh.level_dimensions[0][1]:\n",
    "                valid_y = int((osh.level_dimensions[0][1] - 1 - y) // scalefactor)\n",
    "                tile[valid_y:, :, :] = 255\n",
    "    \n",
    "            # predict without or with padding (reduces boundary effects)\n",
    "            #unet_out = predict_without_padding(tile, model, device)\n",
    "            unet_out = predict_with_padding(tile, model, device)\n",
    "    \n",
    "            # --- START: Added code for saving debug masks ---\n",
    "            # Normalize unet_out to 0-255 for proper image saving, if it's not already in that range\n",
    "            # Assuming unet_out contains class indices (0, 1, 2), scale them for visualization\n",
    "            # You might want to map these to specific colors for better visual distinction\n",
    "            # For example, if 0=background, 1=boundary, 2=internal\n",
    "            display_mask = unet_out * 100 # Simple scaling to make different classes visible\n",
    "            plt.figure(figsize=(6, 6)) # Create a new figure for each plot\n",
    "            plt.imshow(display_mask, cmap='viridis') # Use a colormap to distinguish classes\n",
    "            plt.title(f\"Prediction Mask (x={x}, y={y})\")\n",
    "            plt.colorbar(label='Class Index (Scaled)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(debug_mask_dir, f\"tile_mask_x{x}_y{y}.png\"))\n",
    "            plt.close() # Close the figure to free memory and prevent it from being displayed\n",
    "            # --- END: Added code for saving debug masks ---\n",
    "    \n",
    "            # generate polygons from mask\n",
    "            allinternalpolygons_out += generate_polygons_from_mask(mask=unet_out == 2, coords=[x, y], scalefactor=scalefactor)\n",
    "            allboundarypolygons_out += generate_polygons_from_mask(mask=unet_out == 1, coords=[x, y], scalefactor=scalefactor)\n",
    "    \n",
    "    osh.close()\n",
    "    \n",
    "    # simple fix to avoid issues with splitting in tiles (should be properly fixed with overlapping tiles, and this is rather slow)\n",
    "    # do the union of the polygon with a small positive buffer to make sure that they overlap\n",
    "    def do_union_fix(poly_list):\n",
    "        print('Doing union...')\n",
    "        poly_list = [shapely.buffer(poly, distance=1*scalefactor) for poly in poly_list]\n",
    "        global_poly = shapely.unary_union(poly_list)\n",
    "        # Ensure global_poly is a list of polygons, not a MultiPolygon if it results from unary_union\n",
    "        if global_poly.geom_type == 'MultiPolygon':\n",
    "            poly_list = list(global_poly.geoms)\n",
    "        else:\n",
    "            poly_list = [global_poly] # If it's a single Polygon, put it in a list\n",
    "        return poly_list\n",
    "    \n",
    "    allboundarypolygons_out = do_union_fix(allboundarypolygons_out)\n",
    "    allinternalpolygons_out = do_union_fix(allinternalpolygons_out)\n",
    "    \n",
    "    # generate annotations from polygons with postprocessing\n",
    "    # postprocessing: delete holes, buffer internal, and keep only if surrounded by boundary\n",
    "    allannotations_out = []\n",
    "    colors = mpl.colormaps['tab10'](np.linspace(0, 1, 10))[:, 0:3]\n",
    "    color_annotations = [rgb_to_qupath_color(int(255 * c[0]), int(255 * c[1]), int(255 * c[2])) for c in colors]\n",
    "    searchtree_boundary = STRtree([s for s in allboundarypolygons_out])\n",
    "    for ids_poly, poly in enumerate(tqdm(allinternalpolygons_out)):\n",
    "        # Ensure poly is a valid Polygon before attempting to access exterior/interiors\n",
    "        if not poly.is_valid:\n",
    "            poly = poly.buffer(0) # Attempt to fix invalid polygons\n",
    "            if not poly.is_valid:\n",
    "                print(f\"Skipping invalid polygon after buffer attempt: {poly}\")\n",
    "                continue\n",
    "    \n",
    "        poly = shapely.Polygon(shell=poly.exterior.coords._coords)\n",
    "        poly_buf = shapely.buffer(poly, distance=50)\n",
    "        buf = shapely.difference(poly_buf, poly)\n",
    "    \n",
    "        # Ensure buf is a valid geometry for query\n",
    "        if not buf.is_valid:\n",
    "            buf = buf.buffer(0)\n",
    "            if not buf.is_valid:\n",
    "                print(f\"Skipping invalid buffer geometry: {buf}\")\n",
    "                continue\n",
    "    \n",
    "        hits = searchtree_boundary.query(buf, predicate='intersects')\n",
    "        area_intersect = 0\n",
    "        for hit_idx in hits: # Iterate over indices returned by query\n",
    "            intersect_poly = searchtree_boundary.geometries.take(hit_idx)\n",
    "            # Ensure intersect_poly is valid before intersection\n",
    "            if intersect_poly.is_valid:\n",
    "                intersection_result = shapely.intersection(buf, intersect_poly)\n",
    "                if intersection_result.is_valid:\n",
    "                    area_intersect += intersection_result.area\n",
    "        area_buffer = buf.area\n",
    "        area_poly = poly.area\n",
    "        if area_poly < 100*100 or area_intersect/(area_buffer + 1e-8) < 0.8:\n",
    "            continue\n",
    "        coords = poly_buf.exterior.coords._coords.astype(np.int64).tolist()\n",
    "        coords_int = [interior.coords._coords.astype(np.int64).tolist() for interior in poly_buf.interiors]\n",
    "        ann = {\"geometry\": {\"type\": 'Polygon', \"coordinates\": [coords] + coords_int},\n",
    "               \"properties\": {\"object_type\": \"detection\", \"isLocked\": False,\n",
    "                              'classification': {\"name\": \"Villi\" + str(ids_poly % len(color_annotations)),\n",
    "                                                 \"colorRGB\": color_annotations[ids_poly % len(color_annotations)]},\n",
    "                              \"measurements\": [{\"name\": \"Intersection_rate\", \"value\": str(area_intersect/(area_buffer + 1e-8))}]\n",
    "                              },\n",
    "               \"type\": \"Feature\"\n",
    "               }\n",
    "        allannotations_out.append(ann)\n",
    "    \n",
    "    # compress and save the output\n",
    "    json_fname_output = 'D:\\\\Molar_pregnancy\\\\Aron_to_share\\\\Output_json\\\\DP000020' + str(img_num).zfill(2) + '.json.gz'\n",
    "    os.makedirs(os.path.dirname(json_fname_output), exist_ok=True)\n",
    "    if json_fname_output.endswith(\".gz\"):\n",
    "        with gzip.open(json_fname_output, 'wt', encoding=\"utf-8\") as zipfile:\n",
    "            geojson.dump(allannotations_out, zipfile)\n",
    "    else:\n",
    "        with open(json_fname_output, 'w') as outfile:\n",
    "            geojson.dump(allannotations_out, outfile)\n",
    "    print(\"done saving annotations\")\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Elapsed time for image {img} : {end-start}')\n",
    "\n",
    "print(f\"Done exploring all files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e565c8-119a-47a4-9dcc-1d790d0ed74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
